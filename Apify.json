import { Actor } from 'apify';
import { CheerioCrawler, Dataset } from 'crawlee';

await Actor.init();

const { searchUrls = [] } = await Actor.getInput();
const dataset = await Dataset.open();

const crawler = new CheerioCrawler({
  async requestHandler({ request, $, body }) {
    const platform = request.url.includes('instagram.com') ? 'Instagram' : 'LinkedIn';
    const links = [];

    $('a').each((_, el) => {
      const href = $(el).attr('href');
      if (href && (href.includes('/p/') || href.includes('/in/'))) {
        links.push(href.startsWith('http') ? href : `https://${platform.toLowerCase()}.com${href}`);
      }
    });

    for (const link of links.slice(0, 50)) {
      await dataset.pushData({
        platform,
        source_url: link,
        date_added: new Date().toISOString().split('T')[0],
        status: 'Harvested',
      });
    }
  },
});

const requests = searchUrls.map(url => ({ url }));
await crawler.run(requests);

await Actor.exit();
